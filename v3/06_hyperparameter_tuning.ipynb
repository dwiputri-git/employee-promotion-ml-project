{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 06 - Hyperparameter Tuning (V3)\n",
        "\n",
        "- Tune top models (e.g., GradientBoosting, XGBoost) with Optuna\n",
        "- Objective: PR-AUC with RepeatedStratifiedKFold (5Ã—3)\n",
        "- Save best params and CV metrics to `../v3_artifacts/`\n",
        "- Keep `N_TRIALS` configurable to control runtime\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import numpy as np\n",
        "import optuna\n",
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "INP = Path('../v3_data/employee_promotion_features.csv')\n",
        "ART = Path('../v3_artifacts'); ART.mkdir(exist_ok=True)\n",
        "\n",
        "TARGET = 'Promotion_Eligible'\n",
        "N_SPLITS, N_REPEATS, SEED = 5, 3, 42\n",
        "N_TRIALS = 20  # adjust for depth of search\n",
        "\n",
        "# Data\n",
        "df = pd.read_csv(INP)\n",
        "X = df.drop(columns=[TARGET])\n",
        "y = df[TARGET]\n",
        "\n",
        "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
        "\n",
        "pre = ColumnTransformer([\n",
        "    ('num', StandardScaler(), num_cols),\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
        "])\n",
        "\n",
        "cv = RepeatedStratifiedKFold(n_splits=N_SPLITS, n_repeats=N_REPEATS, random_state=SEED)\n",
        "\n",
        "def tune_gb():\n",
        "    def objective(trial: optuna.Trial):\n",
        "        params = {\n",
        "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "            'n_estimators': trial.suggest_int('n_estimators', 100, 800, step=50),\n",
        "            'max_depth': trial.suggest_int('max_depth', 2, 8),\n",
        "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
        "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
        "            'random_state': SEED,\n",
        "        }\n",
        "        model = GradientBoostingClassifier(**params)\n",
        "        pipe = Pipeline([('pre', pre), ('model', model)])\n",
        "        prauc_scores = []\n",
        "        for tr, va in cv.split(X, y):\n",
        "            pipe.fit(X.iloc[tr], y.iloc[tr])\n",
        "            proba = pipe.predict_proba(X.iloc[va])[:, 1]\n",
        "            prauc_scores.append(average_precision_score(y.iloc[va], proba))\n",
        "        return float(np.mean(prauc_scores))\n",
        "\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=False)\n",
        "    with open(ART / 'tuning_gb.json', 'w') as f:\n",
        "        json.dump({'best_value': study.best_value, 'best_params': study.best_params}, f, indent=2)\n",
        "    return study.best_value, study.best_params\n",
        "\n",
        "\n",
        "def tune_xgb():\n",
        "    def objective(trial: optuna.Trial):\n",
        "        params = {\n",
        "            'n_estimators': trial.suggest_int('n_estimators', 200, 800, step=50),\n",
        "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "            'max_depth': trial.suggest_int('max_depth', 2, 8),\n",
        "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "            'min_child_weight': trial.suggest_float('min_child_weight', 1.0, 20.0),\n",
        "            'random_state': SEED,\n",
        "            'eval_metric': 'logloss',\n",
        "            'n_jobs': -1,\n",
        "        }\n",
        "        model = XGBClassifier(**params)\n",
        "        pipe = Pipeline([('pre', pre), ('model', model)])\n",
        "        prauc_scores = []\n",
        "        for tr, va in cv.split(X, y):\n",
        "            pipe.fit(X.iloc[tr], y.iloc[tr])\n",
        "            proba = pipe.predict_proba(X.iloc[va])[:, 1]\n",
        "            prauc_scores.append(average_precision_score(y.iloc[va], proba))\n",
        "        return float(np.mean(prauc_scores))\n",
        "\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=False)\n",
        "    with open(ART / 'tuning_xgb.json', 'w') as f:\n",
        "        json.dump({'best_value': study.best_value, 'best_params': study.best_params}, f, indent=2)\n",
        "    return study.best_value, study.best_params\n",
        "\n",
        "best_gb = tune_gb()\n",
        "best_xgb = tune_xgb()\n",
        "print('GB best:', best_gb)\n",
        "print('XGB best:', best_xgb)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
